{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9911773,
          "sourceType": "datasetVersion",
          "datasetId": 6090241
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.663037Z",
          "iopub.execute_input": "2024-11-18T00:12:56.663465Z",
          "iopub.status.idle": "2024-11-18T00:12:56.670496Z",
          "shell.execute_reply.started": "2024-11-18T00:12:56.663413Z",
          "shell.execute_reply": "2024-11-18T00:12:56.669467Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBkJ1HzaV0cn",
        "outputId": "7a17aab2-04bf-478d-d13a-0c4079b4c785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_rate=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "        self.dropout = nn.Dropout2d(p=dropout_rate)  # Spatial dropout for convolutional layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout(out)  # Apply dropout\n",
        "\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "class DogHeartCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3, dropout_rate=0.4):\n",
        "        super(DogHeartCNN, self).__init__()\n",
        "        # Initial Convolutional Layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual Blocks\n",
        "        self.layer1 = self._make_layer(64, 64, 3, stride=1, dropout_rate=0.2)\n",
        "        self.layer2 = self._make_layer(64, 128, 3, stride=2, dropout_rate=0.3)\n",
        "        self.layer3 = self._make_layer(128, 256, 3, stride=2, dropout_rate=0.3)\n",
        "        self.layer4 = self._make_layer(256, 512, 3, stride=2, dropout_rate=0.4)\n",
        "\n",
        "        # Adaptive Average Pooling and Fully Connected Layer\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride, dropout_rate):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        layers = [ResidualBlock(in_channels, out_channels, stride, downsample, dropout_rate)]\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial Convolutional Layer\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Residual Layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Adaptive Average Pooling\n",
        "        x = self.avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = DogHeartCNN(num_classes=3).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(model)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.672016Z",
          "iopub.execute_input": "2024-11-18T00:12:56.672309Z",
          "iopub.status.idle": "2024-11-18T00:12:56.870255Z",
          "shell.execute_reply.started": "2024-11-18T00:12:56.672279Z",
          "shell.execute_reply": "2024-11-18T00:12:56.869320Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfH4GY7BV0co",
        "outputId": "b74946d8-8097-44b4-adac-e1fd903b7339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DogHeartCNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (dropout): Dropout2d(p=0.4, inplace=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.4, inplace=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dropout): Dropout2d(p=0.4, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        # Collect image filenames in the directory\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        # Apply transformations (e.g., resizing, normalization)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.image_files[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.871408Z",
          "iopub.execute_input": "2024-11-18T00:12:56.871741Z",
          "iopub.status.idle": "2024-11-18T00:12:56.878706Z",
          "shell.execute_reply.started": "2024-11-18T00:12:56.871708Z",
          "shell.execute_reply": "2024-11-18T00:12:56.877847Z"
        },
        "id": "mDbwafY7V0cp"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define data directories\n",
        "data_dir = '/content/drive/My Drive/'\n",
        "train_dir = os.path.join(data_dir, 'Train')\n",
        "valid_dir = os.path.join(data_dir, 'Valid')\n",
        "test_dir = os.path.join(data_dir, 'Test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_T2nCoIeqS9",
        "outputId": "323762b5-ccd1-496a-b18c-511e0b16a54e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 75x75\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
        "    transforms.RandomRotation(10),  # Random rotation\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.4926, 0.4927, 0.4926], std=[0.2077, 0.2076, 0.2077])  # Normalize\n",
        "])\n",
        "\n",
        "# Test Transformations\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 75x75\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.4926, 0.4927, 0.4926], std=[0.2077, 0.2076, 0.2077])  # Normalize\n",
        "])\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_data = datasets.ImageFolder('/content/drive/My Drive/Train', transform=transform_train)\n",
        "val_data = datasets.ImageFolder('/content/drive/My Drive/Valid', transform=transform_test)\n",
        "test_data = TestDataset('/content/drive/My Drive/Test', transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.880861Z",
          "iopub.execute_input": "2024-11-18T00:12:56.881147Z",
          "iopub.status.idle": "2024-11-18T00:12:56.910375Z",
          "shell.execute_reply.started": "2024-11-18T00:12:56.881116Z",
          "shell.execute_reply": "2024-11-18T00:12:56.909477Z"
        },
        "id": "zBvHRanqV0cp"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Training transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation and Test transformations\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.911653Z",
          "iopub.execute_input": "2024-11-18T00:12:56.912300Z",
          "iopub.status.idle": "2024-11-18T00:12:56.918978Z",
          "shell.execute_reply.started": "2024-11-18T00:12:56.912255Z",
          "shell.execute_reply": "2024-11-18T00:12:56.917935Z"
        },
        "id": "ksVpPVg6V0cp"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: AdamW for better generalization\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
        "\n",
        "# Scheduler: StepLR with step size of 10 epochs and gamma of 0.1\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50, patience=10):\n",
        "    best_val_accuracy = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"Starting training with StepLR scheduler...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        # Print loss and validation accuracy\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"Best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        #if patience_counter >= patience:\n",
        "            #print(\"Early stopping triggered.\")\n",
        "            #break\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50, patience=20)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:12:56.920133Z",
          "iopub.execute_input": "2024-11-18T00:12:56.920442Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ke21sbmV0cp",
        "outputId": "071f59bc-2ee5-4ee6-e3cb-81edceddc445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with StepLR scheduler...\n",
            "Epoch [1/50] - Loss: 0.7750 - Validation Accuracy: 68.50%\n",
            "Best model saved!\n",
            "Epoch [2/50] - Loss: 0.7423 - Validation Accuracy: 67.50%\n",
            "Epoch [3/50] - Loss: 0.7069 - Validation Accuracy: 63.00%\n",
            "Epoch [4/50] - Loss: 0.7277 - Validation Accuracy: 68.50%\n",
            "Epoch [5/50] - Loss: 0.7114 - Validation Accuracy: 70.50%\n",
            "Best model saved!\n",
            "Epoch [6/50] - Loss: 0.6945 - Validation Accuracy: 64.50%\n",
            "Epoch [7/50] - Loss: 0.6656 - Validation Accuracy: 66.00%\n",
            "Epoch [8/50] - Loss: 0.6654 - Validation Accuracy: 66.00%\n",
            "Epoch [9/50] - Loss: 0.6644 - Validation Accuracy: 68.00%\n",
            "Epoch [10/50] - Loss: 0.6683 - Validation Accuracy: 66.50%\n",
            "Epoch [11/50] - Loss: 0.6288 - Validation Accuracy: 68.00%\n",
            "Epoch [12/50] - Loss: 0.6103 - Validation Accuracy: 70.00%\n",
            "Epoch [13/50] - Loss: 0.6146 - Validation Accuracy: 70.00%\n",
            "Epoch [14/50] - Loss: 0.6045 - Validation Accuracy: 69.50%\n",
            "Epoch [15/50] - Loss: 0.5954 - Validation Accuracy: 68.00%\n",
            "Epoch [16/50] - Loss: 0.5920 - Validation Accuracy: 69.50%\n",
            "Epoch [17/50] - Loss: 0.5862 - Validation Accuracy: 69.00%\n",
            "Epoch [18/50] - Loss: 0.6007 - Validation Accuracy: 69.00%\n",
            "Epoch [19/50] - Loss: 0.5730 - Validation Accuracy: 67.50%\n",
            "Epoch [20/50] - Loss: 0.5957 - Validation Accuracy: 67.50%\n",
            "Epoch [21/50] - Loss: 0.5792 - Validation Accuracy: 69.00%\n",
            "Epoch [22/50] - Loss: 0.5742 - Validation Accuracy: 68.00%\n",
            "Epoch [23/50] - Loss: 0.5525 - Validation Accuracy: 68.00%\n",
            "Epoch [24/50] - Loss: 0.5745 - Validation Accuracy: 68.00%\n",
            "Epoch [25/50] - Loss: 0.5752 - Validation Accuracy: 68.00%\n",
            "Epoch [26/50] - Loss: 0.5727 - Validation Accuracy: 68.50%\n",
            "Epoch [27/50] - Loss: 0.5844 - Validation Accuracy: 67.00%\n",
            "Epoch [28/50] - Loss: 0.5643 - Validation Accuracy: 67.50%\n",
            "Epoch [29/50] - Loss: 0.5670 - Validation Accuracy: 68.00%\n",
            "Epoch [30/50] - Loss: 0.5636 - Validation Accuracy: 68.50%\n",
            "Epoch [31/50] - Loss: 0.5705 - Validation Accuracy: 68.00%\n",
            "Epoch [32/50] - Loss: 0.5537 - Validation Accuracy: 68.50%\n",
            "Epoch [33/50] - Loss: 0.5568 - Validation Accuracy: 69.00%\n",
            "Epoch [34/50] - Loss: 0.5838 - Validation Accuracy: 69.00%\n",
            "Epoch [35/50] - Loss: 0.5756 - Validation Accuracy: 68.00%\n",
            "Epoch [36/50] - Loss: 0.5720 - Validation Accuracy: 69.00%\n",
            "Epoch [37/50] - Loss: 0.5791 - Validation Accuracy: 67.50%\n",
            "Epoch [38/50] - Loss: 0.5785 - Validation Accuracy: 67.50%\n",
            "Epoch [39/50] - Loss: 0.5590 - Validation Accuracy: 68.00%\n",
            "Epoch [40/50] - Loss: 0.5687 - Validation Accuracy: 69.00%\n",
            "Epoch [41/50] - Loss: 0.5595 - Validation Accuracy: 69.00%\n",
            "Epoch [42/50] - Loss: 0.5747 - Validation Accuracy: 68.00%\n",
            "Epoch [43/50] - Loss: 0.5786 - Validation Accuracy: 69.00%\n",
            "Epoch [44/50] - Loss: 0.5805 - Validation Accuracy: 68.00%\n",
            "Epoch [45/50] - Loss: 0.5890 - Validation Accuracy: 68.50%\n",
            "Epoch [46/50] - Loss: 0.5730 - Validation Accuracy: 67.50%\n",
            "Epoch [47/50] - Loss: 0.5767 - Validation Accuracy: 68.50%\n",
            "Epoch [48/50] - Loss: 0.5533 - Validation Accuracy: 68.50%\n",
            "Epoch [49/50] - Loss: 0.5550 - Validation Accuracy: 67.50%\n",
            "Epoch [50/50] - Loss: 0.5795 - Validation Accuracy: 69.00%\n",
            "Training completed.\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            for i in range(len(filenames)):\n",
        "                predictions.append((filenames[i], predicted[i].item()))\n",
        "    return predictions\n",
        "\n",
        "# Generate and save predictions\n",
        "predictions = generate_predictions(model, test_loader)\n",
        "\n",
        "# Save the file to the user's home folder\n",
        "output_file = os.path.expanduser('~/test_predictions.csv')  # Saves in the user's home directory\n",
        "pd.DataFrame(predictions, columns=[\"Filename\", \"Predicted Class\"]).to_csv(output_file, index=False, header=False)\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qdCJi6Ou7qr",
        "outputId": "3068f40c-eb39-4f97-cc86-b6c188bc053f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /root/test_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            for i in range(len(filenames)):\n",
        "                predictions.append((filenames[i], predicted[i].item()))\n",
        "    return predictions\n",
        "\n",
        "# Generate and save predictions\n",
        "predictions = generate_predictions(model, test_loader)\n",
        "\n",
        "# Save the file to the Colab's root directory\n",
        "output_file = '/content/test_predictions.csv'  # Path for Colab root directory\n",
        "pd.DataFrame(predictions, columns=[\"Filename\", \"Predicted Class\"]).to_csv(output_file, index=False, header=False)\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGdBT4WEeUc",
        "outputId": "a8fad13a-0174-4193-d1a6-7c84bbcb9782"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/test_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}